
# Homework 3 Part 1

**Due: Wednesday, November 16 @ 11:59 PM**

Problem 1 explores why regularization techniques like lasso or ridge cannot be applied to the k-NN algorithm, concluding that k-NN's reliance on the nearest neighbors' majority voting makes it incompatible with regularization methods that adjust coefficients.

Problem 2 examines the decision surfaces generated by a k-NN algorithm for a two-class dataset, highlighting differences between them and discussing how the k-NN voting system impacts the resulting decision boundaries.

Problem 3 delves into Support Vector Machine (SVM) specifics, identifying support vectors as the critical elements used in predictions and explaining how the regularization parameter ùê∂ influences the SVM model's decision surface, performance, and the number of support vectors.

Problem 4 compares and contrasts the Perceptron, Logistic Regression, Fisher's LDA, and SVM, discussing their assumptions, computational complexity, convergence criteria, and key similarities and differences.

Problem 5 discusses the decision-making process of logistic regression and SVM for binary classification, providing guidance on when one might be preferred over the other based on the nature of the data and the problem at hand.

Problem 6 involves calculating True Positive Rate (TPR) and False Positive Rate (FPR) for various thresholds using a logistic regression classifier's confidence values for a given set of binary labels, drawing an ROC curve from these calculations, and determining an optimal threshold to maintain a specific FPR.

Problem 7 evaluates PCA's effectiveness as a dimensionality reduction technique for classification, highlighting its limitations due to its linear nature and potential to overlook key variables in non-linear data structures.

Problem 8 questions PCA's ability to maintain class separability when reducing dataset dimensions, noting its effectiveness in certain linearly-separable datasets but not in more complex structures, and suggests nonlinear techniques like Kernel PCA for more challenging datasets.

Problem 9 explores uncorrelating a data matrix using PCA, involving calculations for linear transformation, 2-D projection, explained variance, and the covariance matrix of the transformed data, emphasizing the role of eigenvectors and eigenvalues in these processes.

Problem 10 contrasts Fisher's LDA and PCA, pointing out LDA's supervision and focus on class separability versus PCA's unsupervised variance maximization, guiding when one might be preferred over the other based on data labels and objectives.

**This is a individuak assignment.**

Find the assignment description in the file "Homework 3 Part 1.ipynb".
